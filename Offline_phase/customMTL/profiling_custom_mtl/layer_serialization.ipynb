{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-01 12:10:29.592243: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-01 12:10:29.594907: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-01 12:10:29.624046: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-01 12:10:30.080997: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "# from tensorflowkeras.utils.vis_utils import plot_model\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import ndimage\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization,Conv2D,MaxPooling2D,Activation,Dropout,Lambda,Dense,Flatten,Input\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import csv\n",
    "import gc\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_MTL():\n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        self.layer_list=[]\n",
    "        self.weight_set=False\n",
    "        self.partition_done = False\n",
    "        x_shape, y_angle_shape, y_cifar10_shape=(50000, 32, 32, 3), (50000, 4), (50000, 10)\n",
    "        angle_classifier_no=4\n",
    "        cifar10_classifier_no=10\n",
    "        inputs = keras.Input((32, 32, 3))\n",
    "        conv_base = self.get_convbase(inputs)\n",
    "        angle_classifier = self.get_classifier(conv_base, angle_classifier_no, \"angle\",count=15)\n",
    "        cifar10_classifier = self.get_classifier(conv_base, cifar10_classifier_no, \"cifar10\",count=22)\n",
    "        self.model = Model(\n",
    "            inputs=inputs, \n",
    "            outputs=[cifar10_classifier, angle_classifier]\n",
    "        )\n",
    "        tf.keras.utils.plot_model(self.model, to_file='final_mtl_model.png', show_shapes=True,show_layer_names=True)\n",
    "    \n",
    "   \n",
    "    def get_convbase(self,inputs):\n",
    "    \n",
    "    # reg = keras.regularizers.l2(1e-4)\n",
    "    \n",
    "    # initializer = keras.initializers.HeNormal()\n",
    "\n",
    "\n",
    "        x = Conv2D(16, (3, 3), padding=\"same\",name='1_conv2D_1')(inputs)\n",
    "        x = Activation(\"relu\",name='2_activation_1')(x)\n",
    "        x = BatchNormalization(axis=-1,name='3_batch_norm_1')(x)\n",
    "        x = MaxPooling2D(pool_size=(3, 3),name='4_maxPool_1')(x)\n",
    "        x = Dropout(0.25,name='5_dropout_1')(x)\n",
    "        \n",
    "        x = Conv2D(32, (3, 3), padding=\"same\",name='6_conv2D_2')(x)\n",
    "        x = Activation(\"relu\",name='7_activation_2')(x)\n",
    "        x = BatchNormalization(axis=-1,name='8_batch_norm_2')(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2),name='9_maxPool_2')(x)\n",
    "        x = Dropout(0.25,name='10_dropout_2')(x)\n",
    "        \n",
    "        x = Conv2D(32, (3, 3), padding=\"same\",name='11_conv2D_3')(x)\n",
    "        x = Activation(\"relu\",name='12_activation_3')(x)\n",
    "        x = BatchNormalization(axis=-1,name='13_batch_norm_3')(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2),name='14_maxPool_3')(x)\n",
    "        x = Dropout(0.25,name='15_dropout_3')(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_classifier(self,x, class_no, name,count):\n",
    "    \n",
    "        x = Flatten(name=f'{count+1}_layer')(x)\n",
    "        x = Dense(128,name=f'{count+2}_layer')(x)\n",
    "        x = Activation(\"relu\",name=f'{count+3}_layer')(x)\n",
    "        x = BatchNormalization(name=f'{count+4}_layer')(x)\n",
    "        x = Dropout(0.5,name=f'{count+5}_layer')(x)\n",
    "        x = Dense(class_no,name=f'{count+6}_layer')(x)\n",
    "        x = Activation(\"softmax\", name=name)(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def load_weights(self):\n",
    "        self.model.load_weights('./custom_mtl_model.h5')\n",
    "        self.weight_set=True\n",
    "        return\n",
    "    def load_input(self):\n",
    "        self.input_data = np.random.rand(1, 32, 32, 3).astype(np.float32)\n",
    "        self.input_loaded=True\n",
    "        return self.input_data\n",
    "        \n",
    "    def save_pickeled_layers(self):\n",
    "        if not self.weight_set:\n",
    "            self.loadWeights()\n",
    "\n",
    "        \n",
    "        if not self.partition_done:\n",
    "            self.make_partition()\n",
    "        save_dir='./../pickle_layers'\n",
    "        for i in range(len(self.layer_list)):\n",
    "            fname=f'./{save_dir}/custum_mtl_layer_{i}.pkl'\n",
    "            layer_weights_and_config = {\n",
    "                'weights': self.layer_list[i].get_weights(),\n",
    "                'config': tf.keras.layers.serialize(self.layer_list[i])}\n",
    "            with open(fname, 'wb') as f:\n",
    "                pickle.dump(layer_weights_and_config, f)\n",
    "                \n",
    "                \n",
    "    def make_partition(self):\n",
    "        self.layer_list=[]\n",
    "        self.NO_OF_LAYERS= len(self.model.layers)\n",
    "        \n",
    "        for i in range(self.NO_OF_LAYERS):\n",
    "            self.temp_layer=self.model.layers[i]\n",
    "            self.layer_list.append(self.temp_layer)\n",
    "            \n",
    "        self.partition_done = True\n",
    "        print('\\_______Partitioning Done')\n",
    "        \n",
    "    def execute_predict(self,input_data):\n",
    "        st1=time.perf_counter()\n",
    "        out=self.model.predict(input_data)\n",
    "        et1=time.perf_counter()\n",
    "        el1=et1-st1\n",
    "        print(f'Elapsed Time: {el1}')\n",
    "        return out\n",
    "    \n",
    "    def print_lays(self):\n",
    "        for lay in self.model.layers:\n",
    "            self.layer_list.append(lay)\n",
    "            print(lay)\n",
    "        print(f'Total number of Layers : {len(self.layer_list)}')\n",
    "    def execute_lbl(self,input_data):\n",
    "        st2=time.perf_counter()\n",
    "        out=buffer=input_data\n",
    "        for idx in range(1,len(self.model.layers)):\n",
    "            \n",
    "            if idx <= 15:\n",
    "                print(f'Executing index {idx} : {self.model.layers[idx]}')\n",
    "                out=buffer=self.model.layers[idx](out)\n",
    "            elif idx in [16,18,20,22,24,26,28]:\n",
    "                print(f'Executing index {idx} : {self.model.layers[idx]}')\n",
    "                out=self.model.layers[idx](out)\n",
    "            elif idx in [17,19,21,23,25,27,29]:\n",
    "                print(f'Executing index {idx} : {self.model.layers[idx]}')\n",
    "                buffer=self.model.layers[idx](buffer)\n",
    "        et2=time.perf_counter()\n",
    "        el2=et2-st2\n",
    "        print(f'Elapsed Time: {el2}')\n",
    "        return out,buffer\n",
    "    \n",
    "    def gte_input_list(self,input_data):\n",
    "        self.input_list=[]\n",
    "        st2=time.perf_counter()\n",
    "        out=buffer=input_data\n",
    "        self.input_list.append(input_data)\n",
    "        for idx in range(1,len(self.model.layers)):\n",
    "            # print(f'Executing index {idx} :  {self.model.layers[idx]}')\n",
    "            if idx <= 15:\n",
    "                self.input_list.append(out)\n",
    "                out=buffer=self.model.layers[idx](out)\n",
    "            elif idx in [16,18,20,22,24,26,28]:\n",
    "                self.input_list.append(out)\n",
    "                out=self.model.layers[idx](out)\n",
    "            elif idx in [17,19,21,23,25,27,29]:\n",
    "                self.input_list.append(buffer)\n",
    "                buffer=self.model.layers[idx](buffer)\n",
    "        et2=time.perf_counter()\n",
    "        el2=et2-st2\n",
    "        print(f'Elapsed Time: {el2}')\n",
    "        return self.input_list\n",
    "    \n",
    "    def execute_on_core(self,layer_id,input_data):\n",
    "        # dummy_data=dummy_data\n",
    "        # print(self.layer_list[layer_id].name)\n",
    "        self.temp_out=self.layer_list[layer_id](input_data)\n",
    "        \n",
    "        return self.temp_out\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_OF_LAYERS=30\n",
    "NO_OF_CPU=24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_2d_list_to_csv(data, file_name):\n",
    "    \"\"\"\n",
    "    Save a 2D list to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        data (list): The 2D list to be saved.\n",
    "        file_name (str): The name of the CSV file to be created.\n",
    "    \"\"\"\n",
    "    with open(file_name, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerows(data)\n",
    "\n",
    "    print(f'CSV file \"{file_name}\" has been created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean(num_list):\n",
    "\n",
    "    if not num_list:\n",
    "        return None  # Return None if the list is empty\n",
    "\n",
    "    total = sum(num_list)  # Calculate the sum of all numbers in the list\n",
    "    mean = total / len(num_list)  # Calculate the mean by dividing the sum by the number of elements\n",
    "    return mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.random.rand(1,32,32,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "Elapsed Time: 0.09914163395296782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[0.08216629, 0.07917303, 0.0721583 , 0.10970639, 0.10685154,\n",
       "         0.10086906, 0.16356683, 0.08192592, 0.09662664, 0.10695593]],\n",
       "       dtype=float32),\n",
       " array([[0.23763563, 0.22090478, 0.26563695, 0.27582258]], dtype=float32)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj=custom_MTL()\n",
    "obj.load_weights()\n",
    "# out=obj.model.predict(image)\n",
    "# obj.print_lays()\n",
    "obj.execute_predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\_______Partitioning Done\n"
     ]
    }
   ],
   "source": [
    "obj.make_partition()\n",
    "obj.save_pickeled_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 0.008073513978160918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_LIST=obj.gte_input_list(image)\n",
    "len(INPUT_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file \"./readings/single_layer_profile_custom_mtl_for_percentage.csv\" has been created.\n",
      "CSV file \"./readings/single_layer_ave_profile_custom_mtl_for_percentage.csv\" has been created.\n"
     ]
    }
   ],
   "source": [
    "EXP=25\n",
    "dummy=[0]*NO_OF_CPU\n",
    "layers_ex=[dummy]\n",
    "layers_ex_ave=[dummy]\n",
    "for l in range(1,NO_OF_LAYERS):\n",
    "    cpu_ex=[]\n",
    "    cpu_ex_avg=[]\n",
    "    for cpu in range(NO_OF_CPU):\n",
    "        temp=[]\n",
    "        for i in range(EXP):\n",
    "            temp.append(compute_execution_time(obj,'execute_on_core',cpu,l,INPUT_LIST[l]))\n",
    "        \n",
    "        temp1=calculate_mean(temp)\n",
    "        temp2=min(temp)\n",
    "        cpu_ex.append(temp2)\n",
    "        cpu_ex_avg.append(temp1)\n",
    "    time.sleep(0.5)\n",
    "    layers_ex.append(cpu_ex)\n",
    "    layers_ex_ave.append(cpu_ex_avg)\n",
    "\n",
    "\n",
    "save_2d_list_to_csv(layers_ex, \"./readings/single_layer_profile_custom_mtl_for_percentage.csv\")\n",
    "save_2d_list_to_csv(layers_ex_ave, \"./readings/single_layer_ave_profile_custom_mtl_for_percentage.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
